{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Text summarization\n",
    "with Hugging Face Transformers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To install the library in jupyter notebook environment\n",
    "#!pip install transformers"
   ]
  },
  {
   "source": [
    "## Import"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "source": [
    "## Load summarization pipeline"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'transformers.pipelines.text2text_generation.SummarizationPipeline'>\n"
     ]
    }
   ],
   "source": [
    "# load pre-trained summarization pipeline\n",
    "summarizer = pipeline(\"summarization\")\n",
    "print(type(summarizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'SummarizationPipeline' object has no attribute 'max_model_input_sizes'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-3cc569a0b799>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msummarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_model_input_sizes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'SummarizationPipeline' object has no attribute 'max_model_input_sizes'"
     ]
    }
   ],
   "source": [
    "summarizer.max_model_input_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter = \"\"\"\n",
    "Judicial space\n",
    "Regarding space,\n",
    "Bruno Dayez carries out a topographical analysis of the trial which allows us to highlight several characteristics:\n",
    "First, \"the trial takes place in a defined, unchanging and closed place: the courtroom \", which is itself located within the courthouse, which at first glance makes us think of an imposing and austere temple where we have the impression that we are not necessarily welcome and that there is it's not good to live, while for my part, I can assure you that we have very good times.\n",
    "Therefore, the place where the trial takes place is separate from the ordinary world, \"the justice is a particular, autonomous operation that requires its detachment from the everyday world. \"\n",
    "\n",
    "It is therefore a space separated from the secular space of the city.\n",
    "Regarding the publicity of the audiences, these cannot be filmed\n",
    "and disseminated (except for trials of historical interest by\n",
    "example). When the trial takes place at the last instance, it must\n",
    "putting a definitive end to the conflict is what sets it apart from\n",
    "the gear of private revenge.\n",
    "\n",
    "Indeed, imagine it is broadcast on television, it would be subject to\n",
    "recurrent manner in democratic debate, which would be an obstacle to its\n",
    "essential function of social pacification: \"Res judicata pro veritate\n",
    "habetur, \"the saying goes,\" Res judicata is held to be truth. \"\n",
    "So \"except for the few palate rats baited by the smell of\n",
    "sentence \", of which I am a part, are present at the hearing only those who\n",
    "are summoned to appear there. And it is the press that delivers the only echo of what is happening\n",
    "weft within the walls of the palace.\n",
    "Then, concerning the interior space of the trial, it is divided into\n",
    "regions.\n",
    "Each speaker occupies a limited space, it defines the status\n",
    "even of the speaker, the precise role he must fulfill. \"It is forbidden to\n",
    "put in the place of others because this substitution would risk throwing the\n",
    "confusion in the artificial world of the trial. \"\n",
    "\n",
    "Like the auditorium, the courtroom has two differentiated spaces, the separation of which\n",
    "can be materialized by a barrier, a rope or simply a\n",
    "empty space. One, with benches, is for the public and the other for the stage\n",
    "judicial proper.\n",
    "The space is organized symmetrically on either side of the president,\n",
    "whose chair is often slightly raised. The president is surrounded\n",
    "assessors. Then, at the ends, we find the clerk on one side and the\n",
    "the public prosecutor of the other (or the attorney general if the case falls within the jurisdiction of the seats).\n",
    "\n",
    "So, in general, the public prosecutor is at the same level as the court.\n",
    "The question then arises of the balance of power in the spatial organization of the\n",
    "trial and in particular the asymmetrical position of the prosecutor and the lawyer\n",
    "in relation to the judge. This geographical proximity between the prosecutor and the\n",
    "judge could lead one to believe that the rights of the defense and the necessities of\n",
    "repression are not on an equal footing. And finally, always separated\n",
    "of the public, is the bar where witnesses come to testify, \"the past in\n",
    "flashback \" to use the good word of Master VergÃ¨s. And then, on both sides\n",
    "on the other hand, the benches reserved for the accused and his lawyer are distributed, and those\n",
    "reserved for the victim and his lawyer. \"The compartmentalisation of the actors\n",
    "therefore already freezes in the personification of an action: accusing, defending, judging\n",
    "or be judged. \"\n",
    "\n",
    "We can also add that in addition to the very precise function assigned to each\n",
    "actor, the dress, that is to say the dressing, also makes it possible to better identify\n",
    "the various protagonists of the judicial scene.\n",
    "Antoine Garapon assigns him three main functions. A first\n",
    "function of purification, it purifies the ordinary person before this one\n",
    "exercise its institutional role. Here too, there is a desire to mark the\n",
    "break between life and trial. Then, it aims to protect the\n",
    "person who is about to perform the function which is proper to him, in him\n",
    "conferring a feeling of superiority which will release it from violence\n",
    "legitimate which it is called upon to exercise. And finally, it allows to signify the\n",
    "victory of the appearing over the being, of the character over the person, of\n",
    "the institution on the person. Antoine Garapon says it very rightly: \"the dress\n",
    "allows, for the wearer, identification with his character.\n",
    "Contrary to the saying, in the trial it is the dress that makes the judge,\n",
    "the lawyer and the prosecutor.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'summary_text': ' Bruno Dayez says the trial takes place in a defined, unchanging and closed place: the courtroom . The space is organized symmetrically on either side of the president, the clerk and the public prosecutor of the other . Each speaker occupies a limited space, it defines the status of the speaker, the precise role he must fulfill . The public prosecutor is at the same level as the court .'}]"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# Summarize in a text of minimum 30 words and max 130 words\n",
    "# do_sample False to use a greedi decoder: to return a sequence with next word that has a high probability of making sense\n",
    "summarizer(chapter, max_length=130, min_length=30, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "\n",
    "# from selenium import webdriver\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd"
   ]
  },
  {
   "source": [
    "## Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_page(url):\n",
    "    \"\"\"\n",
    "    Function to \n",
    "    - download page \n",
    "    - and parse it with BeautifulSoup\n",
    "    \"\"\"\n",
    "    # download page\n",
    "    response = requests.get(url)\n",
    "    print(url, response.status_code)\n",
    "    \n",
    "    # parse page\n",
    "    soup = bs(response.content, features=\"lxml\")\n",
    "\n",
    "    return soup\n",
    "\n",
    "\n",
    "def get_book_links(soup, base_url) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to get links of the first 25 books \n",
    "    from search page\n",
    "    \"\"\"\n",
    "    # create empty dataframe\n",
    "    cols = [\"title\", \"author\", \"link\", \"cover_img_link\", \"book_id\"]\n",
    "    books_df = pd.DataFrame(columns=cols)\n",
    "\n",
    "    # scrape info\n",
    "    for rank, element in enumerate(soup.find_all(\"li\", attrs={\"class\": \"booklink\"}), start=1):\n",
    "        title = element.find(\"span\", attrs={\"class\": \"title\"}).text\n",
    "        author = element.find(\"span\", attrs={\"class\": \"subtitle\"}).text\n",
    "        link = base_url + element.find(\"a\", attrs={\"class\": \"link\"}).get(\"href\")\n",
    "        cover_img_link = base_url + element.find(\"img\", attrs={\"class\": \"cover-thumb\"}).get(\"src\")\n",
    "        book_id = re.findall(r\"\\d+\", link)[0]\n",
    "        # utf_8_txt_link = f\"{base_url}/files/{book_id}/{book_id}-0.txt\"\n",
    "        books_df.loc[rank] = [title, author, link, cover_img_link, book_id]\n",
    "        \n",
    "    return books_df\n",
    "\n",
    "def get_book_text(link, base_url):\n",
    "    \"\"\"\n",
    "    Function to get book text in plain text UTF-8\n",
    "    \"\"\"\n",
    "    # download book page\n",
    "    soup = download_page(link)\n",
    "\n",
    "    # scrape book text link\n",
    "    book_text_link = base_url + soup.find(\"a\", attrs={\"class\": \"link\", \"type\": \"text/plain\"}).get(\"href\")\n",
    "\n",
    "    # slow down requests frequency to avoid IP ban\n",
    "    time.sleep(random.uniform(2.0, 3.0))\n",
    "\n",
    "    # download book text page\n",
    "    response = requests.get(book_text_link)\n",
    "    print(book_text_link, response.status_code)\n",
    "    \n",
    "    # return book text\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define base url for links\n",
    "base_url = \"https://www.gutenberg.org\"\n",
    "\n",
    "# download page of most popular books\n",
    "url='https://www.gutenberg.org/ebooks/search/?sort_order=downloads'\n",
    "soup = download_page(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "https://www.gutenberg.org/ebooks/search/?query=asimov 200\n"
     ]
    }
   ],
   "source": [
    "# input search query\n",
    "search = input(\"search for books, authors, genre, ...\")\n",
    "\n",
    "# prepare search query url in required format\n",
    "search_book_url = \"https://www.gutenberg.org/ebooks/search/?query=\"\n",
    "search_book_url += \"+\".join(search.split(\" \"))\n",
    "\n",
    "# download page\n",
    "soup = download_page(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                               title  \\\n",
       "1                                              Youth   \n",
       "2  Worlds Within Worlds: The Story of Nuclear Ene...   \n",
       "3                       100 New Yorkers of the 1970s   \n",
       "4  Worlds Within Worlds: The Story of Nuclear Ene...   \n",
       "5                   The Genetic Effects of Radiation   \n",
       "6  Worlds Within Worlds: The Story of Nuclear Ene...   \n",
       "\n",
       "                                   author  \\\n",
       "1                            Isaac Asimov   \n",
       "2                            Isaac Asimov   \n",
       "3                             Max Millard   \n",
       "4                            Isaac Asimov   \n",
       "5  Isaac Asimov and Theodosius Dobzhansky   \n",
       "6                            Isaac Asimov   \n",
       "\n",
       "                                     link  \\\n",
       "1  https://www.gutenberg.org/ebooks/31547   \n",
       "2  https://www.gutenberg.org/ebooks/49819   \n",
       "3  https://www.gutenberg.org/ebooks/17385   \n",
       "4  https://www.gutenberg.org/ebooks/49821   \n",
       "5  https://www.gutenberg.org/ebooks/55738   \n",
       "6  https://www.gutenberg.org/ebooks/49820   \n",
       "\n",
       "                                      cover_img_link  \\\n",
       "1  https://www.gutenberg.org/cache/epub/31547/pg3...   \n",
       "2  https://www.gutenberg.org/cache/epub/49819/pg4...   \n",
       "3  https://www.gutenberg.org/cache/epub/17385/pg1...   \n",
       "4  https://www.gutenberg.org/cache/epub/49821/pg4...   \n",
       "5  https://www.gutenberg.org/cache/epub/55738/pg5...   \n",
       "6  https://www.gutenberg.org/cache/epub/49820/pg4...   \n",
       "\n",
       "                                      utf_8_txt_link  \n",
       "1  https://www.gutenberg.org/files/31547/31547-0.txt  \n",
       "2  https://www.gutenberg.org/files/49819/49819-0.txt  \n",
       "3  https://www.gutenberg.org/files/17385/17385-0.txt  \n",
       "4  https://www.gutenberg.org/files/49821/49821-0.txt  \n",
       "5  https://www.gutenberg.org/files/55738/55738-0.txt  \n",
       "6  https://www.gutenberg.org/files/49820/49820-0.txt  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>author</th>\n      <th>link</th>\n      <th>cover_img_link</th>\n      <th>utf_8_txt_link</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>Youth</td>\n      <td>Isaac Asimov</td>\n      <td>https://www.gutenberg.org/ebooks/31547</td>\n      <td>https://www.gutenberg.org/cache/epub/31547/pg3...</td>\n      <td>https://www.gutenberg.org/files/31547/31547-0.txt</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Worlds Within Worlds: The Story of Nuclear Ene...</td>\n      <td>Isaac Asimov</td>\n      <td>https://www.gutenberg.org/ebooks/49819</td>\n      <td>https://www.gutenberg.org/cache/epub/49819/pg4...</td>\n      <td>https://www.gutenberg.org/files/49819/49819-0.txt</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>100 New Yorkers of the 1970s</td>\n      <td>Max Millard</td>\n      <td>https://www.gutenberg.org/ebooks/17385</td>\n      <td>https://www.gutenberg.org/cache/epub/17385/pg1...</td>\n      <td>https://www.gutenberg.org/files/17385/17385-0.txt</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Worlds Within Worlds: The Story of Nuclear Ene...</td>\n      <td>Isaac Asimov</td>\n      <td>https://www.gutenberg.org/ebooks/49821</td>\n      <td>https://www.gutenberg.org/cache/epub/49821/pg4...</td>\n      <td>https://www.gutenberg.org/files/49821/49821-0.txt</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>The Genetic Effects of Radiation</td>\n      <td>Isaac Asimov and Theodosius Dobzhansky</td>\n      <td>https://www.gutenberg.org/ebooks/55738</td>\n      <td>https://www.gutenberg.org/cache/epub/55738/pg5...</td>\n      <td>https://www.gutenberg.org/files/55738/55738-0.txt</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Worlds Within Worlds: The Story of Nuclear Ene...</td>\n      <td>Isaac Asimov</td>\n      <td>https://www.gutenberg.org/ebooks/49820</td>\n      <td>https://www.gutenberg.org/cache/epub/49820/pg4...</td>\n      <td>https://www.gutenberg.org/files/49820/49820-0.txt</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "search_df = get_book_links(soup)\n",
    "search_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "https://www.gutenberg.org/ebooks/31547 200\n",
      "https://www.gutenberg.org/ebooks/31547.txt.utf-8 200\n"
     ]
    }
   ],
   "source": [
    "# define base url for links\n",
    "base_url = \"https://www.gutenberg.org\"\n",
    "# book_link = search_df.loc[1, \"link\"]\n",
    "# book_text = get_book_text(book_link, base_url)\n",
    "book_text = get_book_text(\"https://www.gutenberg.org/ebooks/31547\", base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "https://www.gutenberg.org/ebooks/64317 200\n",
      "https://www.gutenberg.org/files/64317/64317-0.txt 200\n"
     ]
    }
   ],
   "source": [
    "def get_book_text(link):\n",
    "    \"\"\"\n",
    "    Function to get book text in plain text UTF-8\n",
    "    \"\"\"\n",
    "    # download book page\n",
    "    soup = download_page(link)\n",
    "\n",
    "    # define base url for links\n",
    "    base_url = \"https://www.gutenberg.org\"\n",
    "\n",
    "    # scrape book text link\n",
    "    book_text_link = base_url + soup.find(\"a\", attrs={\"class\": \"link\"}, href=re.compile(r\".txt\")).get(\"href\")\n",
    "\n",
    "    # slow down requests frequency to avoid IP ban\n",
    "    time.sleep(random.uniform(2.0, 3.0))\n",
    "\n",
    "    # download book text page\n",
    "    response = requests.get(book_text_link)\n",
    "    print(book_text_link, response.status_code)\n",
    "    \n",
    "    # return book text\n",
    "    return response.text\n",
    "book_text = get_book_text(\"https://www.gutenberg.org/ebooks/64317\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Ã¯Â»Â¿The Project Gutenberg eBook of The Great Gatsby, by F. Scott Fitzgerald\n\nThis eBook is for the use of anyone anywhere in the United States and\nmost other parts of the world at no cost and with almost no restrictions\nwhatsoever. You may copy it, give it away or re-use it under the terms\nof the Project Gutenberg License included with this eBook or online at\nwww.gutenberg.org. If you are not located in the United States, you\nwill have to check the laws of the country where you are located before\nusing this eBook.\n\nTitle: The Great Gatsby\n\nAuthor: F. Scott Fitzgerald\n\nRelease Date: January 17, 2021 [eBook #64317]\n[Most recently updated: January 24 2021]\n\nLanguage: English\n\nCharacter set encoding: UTF-8\n\nProduced by: Alex Cabal for the Standard Ebooks project, based on a\n             transcription produced for Project Gutenberg Australia.\n\n*** START OF THE PROJECT GUTENBERG EBOOK THE GREAT GATSBY ***\n\n\n\t\t\t   The Great Gatsby\n\t\t\t\t  by\n\t\t\t F. Scott Fitzgerald\n\n"
     ]
    }
   ],
   "source": [
    "print(book_text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find index position where metadata from website ends\n",
    "metadata_end_idx = book_text.rfind(\"***\",0,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'***\\r\\n\\r\\n\\r\\n\\t\\t\\t   The Great Gatsby\\r\\n\\t\\t\\t\\t  by\\r\\n\\t\\t\\t F. Scott Fitzgerald\\r\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "book_text[metadata_end_idx: 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "78716"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "len(book_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-xsum\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-xsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b784d75e93d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msummarizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"summarization\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msummarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbook_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetadata_end_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmetadata_end_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m3500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m130\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
    "summarizer(book_text[metadata_end_idx:metadata_end_idx + 3500], min_length=30, max_length=130, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse with Beautifulsoup\n",
    "# soup = bs(self._data)\n",
    "\n",
    "        # Get listed links\n",
    "        # link_tags = soup.main.find_all(\"a\", attrs={\"class\": \"property-content\"})\n",
    "        # self._links = [link.attrs[\"href\"] for link in link_tags]\n",
    "\n",
    "        # Create webdriver object\n",
    "        driver = webdriver.Firefox()\n",
    "\n",
    "        # Wait 30 ms to navigate to the webpage\n",
    "        driver.implicitly_wait(30)\n",
    "        driver.get(self.page_url)\n",
    "\n",
    "        # When opening the url on Firefox, a pop-up window appears.\n",
    "        # Click on \"Keep browsing\" to get to the actual page.\n",
    "        python_button = driver.find_elements_by_xpath(\n",
    "            \"//button[@id='uc-btn-accept-banner']\"\n",
    "        )[0]\n",
    "        python_button.click()\n",
    "\n",
    "        # Search for all houses and apartment\n",
    "        # 1. Select \"House and apartment\" label\n",
    "        python_label_button = driver.find_elements_by_xpath(\n",
    "            \"//button[@id='propertyTypesDesktop']\"\n",
    "        )[0]\n",
    "        python_label_button.click()\n",
    "        python_house_apartment_button = driver.find_elements_by_xpath(\n",
    "            \"//li[@data-value='HOUSE,APARTMENT']\"\n",
    "        )[0]\n",
    "        python_house_apartment_button.click()\n",
    "\n",
    "        # 2. Click on search\n",
    "        python_search_button = driver.find_elements_by_xpath(\n",
    "            \"//button[@id='searchBoxSubmitButton']\"\n",
    "        )[0]\n",
    "        python_search_button.click()\n",
    "\n",
    "        # 3. Get links of houses and apartment in 5 pages\n",
    "        self._links = []\n",
    "\n",
    "        # Get links for each page\n",
    "        for _ in range(334):\n",
    "            # Initialize attempts count\n",
    "            attempts_count = 0\n",
    "            while attempts_count < 5:\n",
    "                try:\n",
    "                    links_tags = driver.find_elements_by_xpath(\"//a[@class='card__title-link']\")\n",
    "                    self._links.extend([link.get_attribute(\"href\") for link in links_tags])\n",
    "                    break\n",
    "\n",
    "                except:\n",
    "                    attempts_count += 1\n",
    "\n",
    "            # Navigate to next page\n",
    "            python_label_button = driver.find_elements_by_xpath(\n",
    "                \"//a[@class='pagination__link pagination__link--next button button--text button--size-small']\"\n",
    "            )[0]\n",
    "            python_label_button.click()\n",
    "\n",
    "\n",
    "        # print(self._links)\n",
    "        driver.close()\n",
    "\n",
    "soup.find(\"th\", text=re.compile(name, re.IGNORECASE))\n",
    "soup.select_one(\".classified__title\").text.strip().lower()\n",
    "soup.select_one('th:-soup-contains(\"area\")')\n",
    "label.next_sibling.next_sibling.contents[0].strip())\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/files/84/84-0.htm'"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "html = \"\"\"\n",
    "<a href=\"/files/84/84-0.txt\" type=\"text/plain; charset=utf-8\" class=\"link\" title=\"Download\">Plain Text UTF-8</a>\n",
    "\"\"\"\n",
    "soup = bs(html, features=\"lxml\")\n",
    "soup.find(\"a\", attrs={\"class\": \"link\"}, href=re.compile(r\".txt\")).get(\"href\")\n",
    "# soup.select('a[href*=.txt]')\n",
    "# soup.find_all(href=re.find(r'.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALLOWED_EXTENSIONS = {'txt', 'pdf'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'txt'"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "file = \"qs.az.txt\"\n",
    "# split string into list only 1 time and from the right\n",
    "file.rsplit(\".\", 1)[1]"
   ]
  },
  {
   "source": [
    "# Summarize larger text\n",
    "source = https://github.com/nicknochnack/Longform-Summarization-with-Hugging-Face/blob/main/LongSummarization.ipynb"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# 1. load summarization pipeline\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "# 2. download pages\n",
    "URL = \"https://towardsdatascience.com/a-bayesian-take-on-model-regularization-9356116b6457\"\n",
    "URL = \"https://hackernoon.com/will-the-game-stop-with-gamestop-or-is-this-just-the-beginning-2j1x32aa\"\n",
    "\n",
    "r = requests.get(URL)\n",
    "\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "results = soup.find_all(['h1', 'p'])\n",
    "text = [result.text for result in results]\n",
    "ARTICLE = ' '.join(text)\n",
    "ARTICLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. chunk text\n",
    "max_chunk = 500\n",
    "\n",
    "ARTICLE = ARTICLE.replace('.', '.<eos>')\n",
    "ARTICLE = ARTICLE.replace('?', '?<eos>')\n",
    "ARTICLE = ARTICLE.replace('!', '!<eos>')\n",
    "\n",
    "sentences = ARTICLE.split('<eos>')\n",
    "current_chunk = 0 \n",
    "chunks = []\n",
    "for sentence in sentences:\n",
    "    if len(chunks) == current_chunk + 1: \n",
    "        if len(chunks[current_chunk]) + len(sentence.split(' ')) <= max_chunk:\n",
    "            chunks[current_chunk].extend(sentence.split(' '))\n",
    "        else:\n",
    "            current_chunk += 1\n",
    "            chunks.append(sentence.split(' '))\n",
    "    else:\n",
    "        print(current_chunk)\n",
    "        chunks.append(sentence.split(' '))\n",
    "\n",
    "for chunk_id in range(len(chunks)):\n",
    "    chunks[chunk_id] = ' '.join(chunks[chunk_id])\n",
    "\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. summarize text\n",
    "res = summarizer(chunks, max_length=120, min_length=30, do_sample=False)\n",
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join([summ['summary_text'] for summ in res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. output to a file\n",
    "with open('blogsummary.txt', 'w') as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'href=\"{{ url_for(\\'summarize_book\\', book=1) }}\"'"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "book = 1\n",
    "f'href=\"{{{{ url_for(\\'summarize_book\\', book={book}) }}}}\"'\n",
    "# 'href=\"{{ url_for(' + \"'summarize_book'\" + ', book=book) }}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "href=\"{{ url_for('summarize_book', book=book) }}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.summarization import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"The story is staged in the distant future within our own Milky Way Galaxy, approximately in the late 36th century. A portion of the galaxy is filled with terraformed worlds inhabited by interstellar traveling human beings. For 150 years two mighty space powers have intermittently warred with each other: the Galactic Empire and the Free Planets Alliance.\n",
    "Within the Galactic Empire, based on mid 19th century Prussia, an ambitious military genius, Reinhard von MÃ¼sel, later conferred Reinhard von Lohengramm, is rising to power. He is driven by the desire to free his sister Annerose, who was taken by the Kaiser as a concubine. Later, he wants not only to end the corrupt Goldenbaum dynasty but also to defeat the Free Planets Alliance and to unify the whole galaxy under his rule.\n",
    "In the Free Planets Alliance Star Fleet is another genius, Yang Wen Li. He originally aspired to become a historian through a military academy, and joined the tactical division only out of need for tuition money. He was rapidly promoted to an admiral because he demonstrated excellence in military strategy in a number of decisive battles and conflicts. He becomes the archrival of Reinhard, though they highly respect one another. Unlike Reinhard he is better known for his underdog victories and accomplishments in overcoming seemingly impossible odds and mitigating casualties and damages due to military operations.\n",
    "As a historian, Yang often predicts the motives behind his enemies and narrates the rich history of his world and comments on it. One of his famous quotes is: âThere are few wars between good and evil; most are between one good and another good.â\n",
    "Besides the two main heroes, the story is full of vivid characters and intricate politics. All types of characters, from high nobility, admirals and politicians, to common soldiers and farmers, are interwoven into the story. The story frequently switches away from the main heroes to the Unknown Soldier fighting for his life on the battlefield.\n",
    "There is a third neutral power nominally attached to the Galactic Empire called the Phezzan Dominion, a planet-state (city-state on a galactic scale) which trades with both warring powers. There is also a Terraism cult, which claims that humans should go back to Earth, gaining popularity throughout the galaxy. Throughout the story executive political figures of Phezzan in concert with the upper-hierarchy of the Terraism cult orchestrate a number of conspiracies to shift the tide of the galactic war so that it may favor their objectives.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTime to summarize: 0.01 seconds\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'For 150 years two mighty space powers have intermittently warred with each other: the Galactic Empire and the Free Planets Alliance.\\nIn the Free Planets Alliance Star Fleet is another genius, Yang Wen Li. He originally aspired to become a historian through a military academy, and joined the tactical division only out of need for tuition money.\\nThroughout the story executive political figures of Phezzan in concert with the upper-hierarchy of the Terraism cult orchestrate a number of conspiracies to shift the tide of the galactic war so that it may favor their objectives.'"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "# start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# pass text corpus to summarizer\n",
    "summary = summarize(text)\n",
    "\n",
    "# stop timer and compute the execution time\n",
    "end_time = time.time()\n",
    "diff_time = (end_time - start_time)\n",
    "print(f\"\\nTime to summarize: {diff_time:.2f} seconds\")\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTime to summarize: 0.01 seconds\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'For 150 years two mighty space powers have intermittently warred with each other: the Galactic Empire and the Free Planets Alliance.'"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "# start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# pass text corpus to summarizer\n",
    "## ratio = proportion of summary compared with text\n",
    "summary = summarize(text, ratio=0.1)\n",
    "\n",
    "# stop timer and compute the execution time\n",
    "end_time = time.time()\n",
    "diff_time = (end_time - start_time)\n",
    "print(f\"\\nTime to summarize: {diff_time:.2f} seconds\")\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTime to summarize: 0.01 seconds\n21\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'For 150 years two mighty space powers have intermittently warred with each other: the Galactic Empire and the Free Planets Alliance.'"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "# start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# pass text corpus to summarizer\n",
    "## word_count = number of words in summary\n",
    "summary = summarize(text, word_count=25)\n",
    "\n",
    "# stop timer and compute the execution time\n",
    "end_time = time.time()\n",
    "diff_time = (end_time - start_time)\n",
    "print(f\"\\nTime to summarize: {diff_time:.2f} seconds\")\n",
    "print(len(summary.split(' ')))\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|ââââââââââ| 1.20k/1.20k [00:00<00:00, 202kB/s]\n",
      "Downloading: 100%|ââââââââââ| 242M/242M [00:24<00:00, 10.1MB/s]\n",
      "Downloading: 100%|ââââââââââ| 792k/792k [00:00<00:00, 1.19MB/s]\n",
      "Downloading: 100%|ââââââââââ| 1.39M/1.39M [00:01<00:00, 1.14MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Importing requirements\n",
    "from transformers import T5Tokenizer, T5Config, T5ForConditionalGeneration\n",
    "\n",
    "\n",
    "# Instantiate model and tokenizer \n",
    "my_model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefix string \"summarize:\" to original text\n",
    "text_t5 = \"summarize:\" + text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T5 is a an encoder-decoder model \n",
    "# => convert text into input-ids (=sequence of ids): encode text\n",
    "input_ids=tokenizer.encode(text_t5, return_tensors='pt', max_length=512, truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[    0,     8,   733,    19,  1726,    26,    16,     8, 10382,   647,\n",
       "           441,    69,   293, 18389,    63,  5994, 24856,     3,     5,     8]])"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "# Generate summary ids\n",
    "summary_ids = my_model.generate(input_ids)\n",
    "summary_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<pad> the story is staged in the distant future within our own Milky Way galaxy. the\n"
     ]
    }
   ],
   "source": [
    "# decode tensor of input-ids (inverse of encode())\n",
    "t5_summary = tokenizer.decode(summary_ids[0])\n",
    "print(t5_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "len(t5_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer, BartConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|ââââââââââ| 899k/899k [00:01<00:00, 802kB/s]\n",
      "Downloading: 100%|ââââââââââ| 456k/456k [00:00<00:00, 694kB/s]\n",
      "Downloading: 100%|ââââââââââ| 1.36M/1.36M [00:01<00:00, 959kB/s] \n",
      "Downloading: 100%|ââââââââââ| 1.40k/1.40k [00:00<00:00, 1.35MB/s]\n",
      "Downloading: 100%|ââââââââââ| 1.63G/1.63G [02:50<00:00, 9.55MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer for bart-large-cnn\n",
    "# pretrained model fine tuned for summarization task\n",
    "\n",
    "tokenizer=BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "model=BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'input_ids': tensor([[    0,   133,   527,    16, 10899,    11,     5, 13258,   499,   624,\n            84,   308, 36713,  4846,  5325,     6,  2219,    11,     5,   628,\n          2491,   212,  3220,     4,    83,  4745,     9,     5, 22703,    16,\n          3820,    19,  8470,   763, 10312, 14490, 36308,    30, 43240,  5796,\n          1050, 14766,     4,   286,  3982,   107,    80, 23514,   980,  4361,\n            33, 41870,  7240,   997,  2050,    19,   349,    97,    35,     5,\n         40332, 11492,     8,     5,  3130,  5427,  2580,  6035,     4, 50118,\n         35469,     5, 40332, 11492,     6,   716,    15,  1084,   753,   212,\n          3220,  2869, 17280,     6,    41,  8263,   831, 16333,     6, 17789,\n          9635,  5689, 28168,  5317,     6,   423, 35851, 17789,  9635,  5689,\n          5463, 31075,  4040,   119,     6,    16,  2227,     7,   476,     4,\n            91,    16,  3185,    30,     5,  4724,     7,   481,    39,  2761,\n           660,  1396,  3876,     6,    54,    21,   551,    30,     5, 20838,\n            25,    10, 10146,  1792,   833,     4,  6811,     6,    37,  1072,\n            45,   129,     7,   253,     5, 10334,  3274, 19898, 27284,    53,\n            67,     7,  3002,     5,  3130,  5427,  2580,  6035,     8,     7,\n           542,  4591,     5,  1086, 22703,   223,    39,  2178,     4, 50118,\n          1121,     5,  3130,  5427,  2580,  6035,  2141, 13727,    16,   277,\n         16333,     6, 13262, 19608,  5991,     4,    91,  3249,    25, 25002,\n             7,   555,    10, 17089,   149,    10,   831, 11756,     6,     8,\n          1770,     5, 15714,  2757,   129,    66,     9,   240,    13, 12263,\n           418,     4,    91,    21,  6042,  7715,     7,    41, 20087, 24811,\n           142,    37,  7646, 12411,    11,   831,  1860,    11,    10,   346,\n             9, 12703,  9531,     8,  9549,     4,    91,  3374,     5,  9599,\n         27978,     9, 17789,  9635,     6,   600,    51,  2200,  2098,    65,\n           277,     4,  8280, 17789,  9635,    37,    16,   357,   684,    13,\n            39, 23449,  8156,     8, 15052,    11, 23968,  6590,  4703,  5995,\n             8, 31904, 12675,     8,  8357,   528,     7,   831,  1414,     4,\n         50118,  1620,    10, 17089,     6, 13262,   747, 17876,     5, 22834,\n           639,    39, 11058,     8, 33316,  1626,     5,  4066,   750,     9,\n            39,   232,     8,  1450,    15,    24,     4,   509,     9,    39,\n          3395, 11495,    16,    35,    44,    48,   970,    32,   367,  9425,\n           227,   205,     8,  9247,   131,   144,    32,   227,    65,   205,\n             8,   277,   205,     4,    17,    46, 50118, 41107,     5,    80,\n          1049, 10954,     6,     5,   527,    16,   455,     9, 24106,  3768,\n             8, 25210,  2302,     4,   404,  3505,     9,  3768,     6,    31,\n           239, 45560,     6, 20087,   853,  1536,     8,  3770,     6,     7,\n          1537,  3878,     8,  3111,     6,    32,  3222,   605, 23733,    88,\n             5,   527,     4,    20,   527,  5705, 21737,   409,    31,     5,\n          1049, 10954,     7,     5, 28283, 29845,  2190,    13,    39,   301,\n            15,     5, 20707,     4, 50118,   970,    16,    10,   371,  7974,\n           476, 22693, 30672,  7391,     7,     5, 40332, 11492,   373,     5,\n           221,   700,  7399,   260, 21694,     6,    10,  5518,    12,  4897,\n            36, 14853,    12,  4897,    15,    10, 44047,  3189,    43,    61,\n          4274,    19,   258,   997,  4506,  4361,     4,   345,    16,    67,\n            10, 29871,  1809, 12308,     6,    61,  1449,    14,  5868,   197,\n           213,   124,     7,  3875,     6,  8079,  7347,  1328,     5, 22703,\n             4, 13231,     5,   527,  1031,   559,  2415,     9,   221,   700,\n          7399,   260,    11,  4192,    19,     5,  2853,    12,   298,   906,\n         37208,     9,     5, 29871,  1809, 12308, 30062, 35817,    10,   346,\n             9, 31150, 15668,     7,  3294,     5, 13260,     9,     5, 44047,\n           997,    98,    14,    24,   189,  4402,    49, 10366,     4, 50118,\n             2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# Encode inputs\n",
    "inputs: dict = tokenizer.batch_encode_plus([text],return_tensors='pt')\n",
    "print(inputs)\n",
    "\n",
    "# Generate ids\n",
    "summary_ids = model.generate(inputs['input_ids'], early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The story is staged in the distant future within our own Milky Way Galaxy, approximately in the late 36th century. For 150 years two mighty space powers have intermittently warred with each other: the Galactic Empire and the Free Planets Alliance. The story frequently switches away from the main heroes to the Unknown Soldier fighting for his life on the battlefield.\n"
     ]
    }
   ],
   "source": [
    "# Decode summary\n",
    "bart_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(bart_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|ââââââââââ| 1.04M/1.04M [00:01<00:00, 831kB/s]\n",
      "Downloading: 100%|ââââââââââ| 456k/456k [00:01<00:00, 335kB/s]\n",
      "Downloading: 100%|ââââââââââ| 1.36M/1.36M [00:02<00:00, 643kB/s]\n",
      "Downloading: 100%|ââââââââââ| 665/665 [00:00<00:00, 576kB/s]\n",
      "Downloading: 100%|ââââââââââ| 548M/548M [00:56<00:00, 9.73MB/s]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 512, but ``max_length`` is set to 20.This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    }
   ],
   "source": [
    "# Importing model and tokenizer\n",
    "from transformers import GPT2Tokenizer,GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the model and tokenizer with gpt-2\n",
    "tokenizer=GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model=GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 512, but ``max_length`` is set to 200.This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    }
   ],
   "source": [
    "# Encoding text to get input ids & pass them to model.generate()\n",
    "inputs=tokenizer.batch_encode_plus([text],return_tensors='pt',max_length=512, truncation=True)\n",
    "summary_ids=model.generate(inputs['input_ids'],early_stopping=True, max_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The story is staged in the distant future within our own Milky Way Galaxy, approximately in the late 36th century. A portion of the galaxy is filled with terraformed worlds inhabited by interstellar traveling human beings. For 150 years two mighty space powers have intermittently warred with each other: the Galactic Empire and the Free Planets Alliance.\nWithin the Galactic Empire, based on mid 19th century Prussia, an ambitious military genius, Reinhard von MÃ¼sel, later conferred Reinhard von Lohengramm, is rising to power. He is driven by the desire to free his sister Annerose, who was taken by the Kaiser as a concubine. Later, he wants not only to end the corrupt Goldenbaum dynasty but also to defeat the Free Planets Alliance and to unify the whole galaxy under his rule.\nIn the Free Planets Alliance Star Fleet is another genius, Yang Wen Li. He originally aspired to become a historian through a military academy, and joined the tactical division only out of need for tuition money. He was rapidly promoted to an admiral because he demonstrated excellence in military strategy in a number of decisive battles and conflicts. He becomes the archrival of Reinhard, though they highly respect one another. Unlike Reinhard he is better known for his underdog victories and accomplishments in overcoming seemingly impossible odds and mitigating casualties and damages due to military operations.\nAs a historian, Yang often predicts the motives behind his enemies and narrates the rich history of his world and comments on it. One of his famous quotes is: âThere are few wars between good and evil; most are between one good and another good.â\nBesides the two main heroes, the story is full of vivid characters and intricate politics. All types of characters, from high nobility, admirals and politicians, to common soldiers and farmers, are interwoven into the story. The story frequently switches away from the main heroes to the Unknown Soldier fighting for his life on the battlefield.\nThere is a third neutral power nominally attached to the Galactic Empire called the Phezzan Dominion, a planet-state (city-state on a galactic scale) which trades with both warring powers. There is also a Terraism cult, which claims that humans should go back to Earth, gaining popularity throughout the galaxy. Throughout the story executive political figures of Phezzan in concert with the upper-hierarchy of the Terraism cult orchestrate a number of conspiracies to shift the tide of the galactic war so that\n"
     ]
    }
   ],
   "source": [
    "# Decode\n",
    "GPT_summary=tokenizer.decode(summary_ids[0],skip_special_tokens=True)\n",
    "print(GPT_summary)"
   ]
  },
  {
   "source": [
    "### Summarization with XLM Transformers\n",
    "Summary from XLM not very good. as model was not fine tuned for Summarization."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|ââââââââââ| 646k/646k [00:01<00:00, 531kB/s]\n",
      "Downloading: 100%|ââââââââââ| 487k/487k [00:00<00:00, 501kB/s]\n",
      "Downloading: 100%|ââââââââââ| 840/840 [00:00<00:00, 498kB/s]\n",
      "Downloading: 100%|ââââââââââ| 2.67G/2.67G [04:14<00:00, 10.5MB/s]\n",
      "Some weights of XLMWithLMHeadModel were not initialized from the model checkpoint at xlm-mlm-en-2048 and are newly initialized: ['transformer.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'original_text' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-f79a4087f837>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Encoding text to get input ids & pass them to model.generate()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_encode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moriginal_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0msummary_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'original_text' is not defined"
     ]
    }
   ],
   "source": [
    "# Importing model and tokenizer\n",
    "from transformers import XLMWithLMHeadModel, XLMTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of XLMWithLMHeadModel were not initialized from the model checkpoint at xlm-mlm-en-2048 and are newly initialized: ['transformer.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Input length of input_ids is 512, but ``max_length`` is set to 20.This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "the story is staged in the distant future within our own milky way galaxy, approximately in the late 36th century. a portion of the galaxy is filled with terraformed worlds inhabited by interstellar traveling human beings. for 150 years two mighty space powers have intermittently warred with each other : the galactic empire and the free planets alliance.within the galactic empire, based on mid 19th century prussia, an ambitious military genius, reinhard von musel, later conferred reinhard von lohengramm, is rising to power. he is driven by the desire to free his sister annerose, who was taken by the kaiser as a concubine. later, he wants not only to end the corrupt goldenbaum dynasty but also to defeat the free planets alliance and to unify the whole galaxy under his rule.in the free planets alliance star fleet is another genius, yang wen li. he originally aspired to become a historian through a military academy, and joined the tactical division only out of need for tuition money. he was rapidly promoted to an admiral because he demonstrated excellence in military strategy in a number of decisive battles and conflicts. he becomes the archrival of reinhard, though they highly respect one another. unlike reinhard he is better known for his underdog victories and accomplishments in overcoming seemingly impossible odds and mitigating casualties and damages due to military operations.as a historian, yang often predicts the motives behind his enemies and narrates the rich history of his world and comments on it. one of his famous quotes is : \" there are few wars between good and evil ; most are between one good and another good. \" besides the two main heroes, the story is full of vivid characters and intricate politics. all types of characters, from high nobility, admirals and politicians, to common soldiers and farmers, are interwoven into the story. the story frequently switches away from the main heroes to the unknown soldier fighting for his life on the battlefield.there is a third neutral power nominally attached to the galactic empire called the phezzan dominion, a planet-state ( city-state on a galactic scale ) which trades with both warring powers. there is also a terraism cult, which claims that humans should go back to earth, gaining popularity throughout the galaxy. throughout the story executive political figures of phezzan in concert with the upper-hierarchy of the terraism cult orchestrate a number of conspiracies to shift the tide of the galactic war so that it may\n"
     ]
    }
   ],
   "source": [
    "# Instantiating the model and tokenizer \n",
    "tokenizer=XLMTokenizer.from_pretrained('xlm-mlm-en-2048')\n",
    "model=XLMWithLMHeadModel.from_pretrained('xlm-mlm-en-2048')\n",
    "\n",
    "# Encoding text to get input ids & pass them to model.generate()\n",
    "inputs=tokenizer.batch_encode_plus([text],return_tensors='pt',max_length=512)\n",
    "summary_ids=model.generate(inputs['input_ids'],early_stopping=True)\n",
    "\n",
    "# Decode and print the summary\n",
    "XLM_summary=tokenizer.decode(summary_ids[0],skip_special_tokens=True)\n",
    "print(XLM_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'xlm-mlm-en-2048': 512,\n",
       " 'xlm-mlm-ende-1024': 512,\n",
       " 'xlm-mlm-enfr-1024': 512,\n",
       " 'xlm-mlm-enro-1024': 512,\n",
       " 'xlm-mlm-tlm-xnli15-1024': 512,\n",
       " 'xlm-mlm-xnli15-1024': 512,\n",
       " 'xlm-clm-enfr-1024': 512,\n",
       " 'xlm-clm-ende-1024': 512,\n",
       " 'xlm-mlm-17-1280': 512,\n",
       " 'xlm-mlm-100-1280': 512}"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "tokenizer.max_model_input_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformersTextSummarizer(BaseTextSummarizer):\n",
    "    def __init__ (self, model_key, language):\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(model_key)\n",
    "\n",
    "        self._language = language\n",
    "\n",
    "        self._model = AutoModelForSeq2SeqLM.from_pretrained(model_key)\n",
    "\n",
    "        self._device = 'cuda' if bool(strtobool(os.getenv('USE_GPU'))) else 'cpu'\n",
    "\n",
    "    def __chunk_text(self, text):\n",
    "        sentences = [ s + ' ' for s in sentence_segmentation(text, minimum_n_words_to_accept_sentence=1, language=self._language) ]\n",
    "\n",
    "        chunks = []\n",
    "\n",
    "        chunk = ''\n",
    "\n",
    "        length = 0\n",
    "\n",
    "        for sentence in sentences:\n",
    "            tokenized_sentence = self._tokenizer.encode(sentence, truncation=False, max_length=None, return_tensors='pt') [0]\n",
    "\n",
    "            if len(tokenized_sentence) > self._tokenizer.model_max_length:\n",
    "                continue\n",
    "\n",
    "            length += len(tokenized_sentence)\n",
    "\n",
    "            if length <= self._tokenizer.model_max_length:\n",
    "                chunk = chunk + sentence\n",
    "            else:\n",
    "                chunks.append(chunk.strip())\n",
    "                chunk = sentence\n",
    "                length = len(tokenized_sentence)\n",
    "\n",
    "        if len(chunk) > 0:\n",
    "            chunks.append(chunk.strip())\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def __clean_text(self, text):\n",
    "      if text.count('.') == 0:\n",
    "        return text.strip()\n",
    "\n",
    "      end_index = text.rindex('.') + 1\n",
    "\n",
    "      return text[0 : end_index].strip()\n",
    "\n",
    "    def summarize(self, text, *args, **kwargs):\n",
    "        chunk_texts = self.__chunk_text(text)\n",
    "\n",
    "        chunk_summaries = []\n",
    "\n",
    "        for chunk_text in chunk_texts:\n",
    "            input_tokenized = self._tokenizer.encode(chunk_text, return_tensors='pt')\n",
    "\n",
    "            input_tokenized = input_tokenized.to(self._device)\n",
    "\n",
    "            summary_ids = self._model.to(self._device).generate(input_tokenized, length_penalty=3.0, min_length = int(0.2 * len(chunk_text)), max_length = int(0.3 * len(chunk_text)), early_stopping=True, num_beams=5, no_repeat_ngram_size=2)\n",
    "\n",
    "            output = [self._tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in summary_ids]\n",
    "\n",
    "            chunk_summaries.append(output)\n",
    "\n",
    "        summaries = [ self.__clean_text(text) for chunk_summary in chunk_summaries for text in chunk_summary ]\n",
    "\n",
    "        return summaries\n",
    "\n",
    "def sentence_segmentation(document, minimum_n_words_to_accept_sentence, language):\n",
    "    paragraphs = list(filter(lambda o: len(o.strip()) > 0, document.split('\\n')))\n",
    "\n",
    "    paragraphs = [ p.strip() for p in paragraphs ]\n",
    "\n",
    "    paragraph_sentences = [ sent_tokenize(p, language=language) for p in paragraphs ]\n",
    "\n",
    "    paragraph_sentences = chain(*paragraph_sentences)\n",
    "\n",
    "    paragraph_sentences = [ s.strip() for s in paragraph_sentences ]\n",
    "\n",
    "    normal_word_tokenizer = RegexpTokenizer(r'[^\\W_]+')\n",
    "\n",
    "    paragraph_sentences = filter(lambda o: len(normal_word_tokenizer.tokenize(o)) >= minimum_n_words_to_accept_sentence, paragraph_sentences)\n",
    "\n",
    "    return list(paragraph_sentences)"
   ]
  },
  {
   "source": [
    "My approach is to âexplodeâ the given dataset input in sentences, use the transformer tokenizer to get the length of each sentence and calculate a nice chunking (uniform length, no split sentences). This is the function that I am using:\n",
    "where RE_SPLITTER is â.(?!\\d)|\\nâ"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, num_tok):\n",
    "    text_sent =\\\n",
    "        [sent.strip()+'.' for sent in re.split(RE_SPLITTER, text) if len(sent) > 1]\n",
    "\n",
    "    # calculate number of tokens per sentence\n",
    "    num_tok_sent = [len(tokenizer.tokenize(sent)) for sent in text_sent]\n",
    "    \n",
    "    # calculate chunk dimension to fit into model\n",
    "    n = int(np.ceil(num_tok / MODEL_MAX_LEN))\n",
    "    len_chunk = int(num_tok / n)\n",
    "\n",
    "    # get a more uniform splitting to avoid splits\n",
    "    # which are too short at the end\n",
    "    if len_chunk+50 > MODEL_MAX_LEN:\n",
    "        len_chunk = int(num_tok / (n+1))\n",
    "    \n",
    "    len_curr = 0\n",
    "    text_curr = []\n",
    "    text_chunk = []\n",
    "    for te, len_sent in zip(text_sent, num_tok_sent):\n",
    "\n",
    "        if len_curr + len_sent < len_chunk:\n",
    "            text_curr.append(te)\n",
    "            len_curr += len_sent\n",
    "\n",
    "        elif len_curr + len_sent >= MODEL_MAX_LEN:\n",
    "            text_chunk.append(text_curr)\n",
    "\n",
    "            text_curr = [te]\n",
    "            len_curr = len_sent\n",
    "\n",
    "        else: # >= len_chunk && < MODEL_MAX_LEN\n",
    "            text_curr.append(te)\n",
    "            text_chunk.append(text_curr)\n",
    "            \n",
    "            text_curr = []\n",
    "            len_curr = 0\n",
    "\n",
    "    if len_curr > 0:\n",
    "        text_chunk.append(text_curr)\n",
    "\n",
    "    return text_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2403 > 1024). Running this sequence through the model will result in indexing errors\n",
      "This is a very very long text. This is avery very long texts. This has been a very long day for me. I'm going to have to take a break from this. I've got a lot of work to do. I'll be back in a few days.\n",
      "This is a very very long text. This is avery very long texts. This has been a very long day for me. I'm going to have to take a break from this. I've got a lot of work to do. I'll be back in a few days.\n",
      "This is a very very long text. This is avery very long texts. This has been a very, very long day. This will be a very long, very, long night. I hope you enjoy it. I will be back in a week or so with a new text.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "long_text = \"This is a very very long text. \" * 300\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "# tokenize without truncation\n",
    "inputs_no_trunc = tokenizer(long_text, max_length=None, return_tensors='pt', truncation=False)\n",
    "\n",
    "# get batches of tokens corresponding to the exact model_max_length\n",
    "chunk_start = 0\n",
    "chunk_end = tokenizer.model_max_length  # == 1024 for Bart\n",
    "inputs_batch_lst = []\n",
    "while chunk_start <= len(inputs_no_trunc['input_ids'][0]):\n",
    "    inputs_batch = inputs_no_trunc['input_ids'][0][chunk_start:chunk_end]  # get batch of n tokens\n",
    "    inputs_batch = torch.unsqueeze(inputs_batch, 0)\n",
    "    inputs_batch_lst.append(inputs_batch)\n",
    "    chunk_start += tokenizer.model_max_length  # == 1024 for Bart\n",
    "    chunk_end += tokenizer.model_max_length  # == 1024 for Bart\n",
    "\n",
    "# generate a summary on each batch\n",
    "summary_ids_lst = [model.generate(inputs, num_beams=4, max_length=100, early_stopping=True) for inputs in inputs_batch_lst]\n",
    "\n",
    "# decode the output and join into one string with one paragraph per summary batch\n",
    "summary_batch_lst = []\n",
    "for summary_id in summary_ids_lst:\n",
    "    summary_batch = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_id]\n",
    "    summary_batch_lst.append(summary_batch[0])\n",
    "summary_all = '\\n'.join(summary_batch_lst)\n",
    "\n",
    "print(summary_all)\n",
    "\n",
    "# output (would of course make more sense on a sensible input):\n",
    "#This is a very very long text. This is avery very long texts. This has been a very long day for me. I'm going to have to take a break from this. I've got a lot of work to do. I'll be back in a few days.\n",
    "#This is a very very long text. This is avery very long texts. This has been a very long day for me. I'm going to have to take a break from this. I've got a lot of work to do. I'll be back in a few days.\n",
    "#This is a very very long text. This is avery very long texts. This has been a very, very long day. This will be a very long, very, long night. I hope you enjoy it. I will be back in a week or so with a new text."
   ]
  },
  {
   "source": [
    "The main advantage of this approach is that it uses the tokenization directly from the transformers tokenizer instead of an external tokenizer like NLTK. Keep in mind that most transformer models use different sub-word tokenizers, while NLTK probably uses a word-level tokenizer (see explanation here). This means that NLTK will split a string like âI have a new GPU!â into 6 tokens (one per word + punctuation), while e.g. BERTâs tokenizer will split it into 7 (['i', 'have', 'a', 'new', 'gp', '##u', '!']), because it splits rare words into sub-words (e.g. GPU). With the âpure transformersâ approach you can be sure to really get the exact maximum number of tokens.\n",
    "\n",
    "The disadvantage is that there is no sentence boundary detection. You can theoretically solve that with the NLTK (or SpaCy) approach and splitting sentences. But the token threshold should probably be set below 1024 words (maybe 900?), because 1024 NLTK word tokens translate into more than 1024++ sub-word tokens. Otherwise, the text gets truncated again and you effectively delete parts of your text.\n",
    "\n",
    "I feel like summarising texts above 1024 tokens is probably a common use case and enabling this kind of âlong text summarisationâ could be a very useful feature for the summarisation pipeline. Could this maybe be something that could be added to the pipeline with e.g. a keyword argument like âsummarise_long_text=Trueâ?\n",
    "I dont know the internals of the pipeline well enough to know if this would be an easy addition or too complicated @sshleifer (Also please correct me if Iâm wrong about the code and explanation above, Iâm also new to this)\n",
    "\n",
    "(Btw, look at the input and the outputâ¦ :smiley: is Bart getting lazy when the text is too long and monotonous? :stuck_out_tongue: )"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from typing import List\n",
    "\n",
    "def old_summarization_pipeline(text: List[str]) -> List[str]:\n",
    "    tokenizer = BartTokenizer.from_pretrained('bart-large-cnn')\n",
    "    model = BartForConditionalGeneration.from_pretrained('bart-large-cnn')\n",
    "    input_ids = tokenizer.batch_encode_plus(text, return_tensors='pt', max_length=1024)['input_ids']\n",
    "    summary_ids = model.generate(input_ids)\n",
    "    summaries = [tokenizer.decode(s) for s in summary_ids]\n",
    "    return summaries"
   ]
  },
  {
   "source": [
    "@sshleifer what's the typical recommendation for summarization on larger documents? Chunk them and generate summaries or any other tips?\n",
    "\n",
    "EDIT: Cross-posted here, I think this is a much better place for this.\n",
    "\n",
    "This is what I use currently but open to better recommendations."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# generate chunks of text \\ sentences <= 1024 tokens\n",
    "def nest_sentences(document):\n",
    "  nested = []\n",
    "  sent = []\n",
    "  length = 0\n",
    "  for sentence in nltk.sent_tokenize(document):\n",
    "    length += len(sentence)\n",
    "    if length < 1024:\n",
    "      sent.append(sentence)\n",
    "    else:\n",
    "      nested.append(sent)\n",
    "      sent = [sentence]\n",
    "      length = len(sentence)\n",
    "\n",
    "  if sent:\n",
    "    nested.append(sent)\n",
    "  return nested\n",
    "\n",
    "# generate summary on text with <= 1024 tokens\n",
    "def generate_summary(nested_sentences):\n",
    "  device = 'cuda'\n",
    "  summaries = []\n",
    "  for nested in nested_sentences:\n",
    "    input_tokenized = bart_tokenizer.encode(' '.join(nested), truncation=True, return_tensors='pt')\n",
    "    input_tokenized = input_tokenized.to(device)\n",
    "    summary_ids = bart_model.to(device).generate(input_tokenized,\n",
    "                                      length_penalty=3.0,\n",
    "                                      min_length=30,\n",
    "                                      max_length=100)\n",
    "    output = [bart_tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]\n",
    "    summaries.append(output)\n",
    "  summaries = [sentence for sublist in summaries for sentence in sublist]\n",
    "  return summaries"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "The task does not provide any default models for options ('fr', 'en')",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-243fca3e2991>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtranslator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"translation_fr_to_en\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/transformers/pipelines/__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, framework, revision, use_fast, use_auth_token, model_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0;31m# At that point framework might still be undetermined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_default_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargeted_task\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframework\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0;31m# Try to infer tokenizer from model or config name (if provided as str)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mget_default_model\u001b[0;34m(targeted_task, framework, task_options)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtask_options\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtask_options\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The task does not provide any default models for options {task_options}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0mdefault_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask_options\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m\"model\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The task does not provide any default models for options ('fr', 'en')"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\"translation_fr_to_en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}